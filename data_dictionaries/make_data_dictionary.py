#!/usr/bin/env python3
"""
Data Dictionary Pipeline

Converts ASCR data dictionary (xlsx) into curation-ready artifacts:
1. curation_schema.yaml - Human-readable field definitions
2. curation_models.py - Generated pydantic models with validators
3. curation_schema.jsonc - JSON schema for frontend forms
4. curation_instructions/llm_curation_instructions.md - LLM curation instructions
"""

import argparse
import json
import re
from pathlib import Path
from collections import defaultdict

import yaml
from openpyxl import load_workbook


# Column indices in the xlsx
COLS = {
    'django_class_name': 0,
    'django_field_name': 1,
    'description': 4,
    'key': 5,
    'data_type': 6,
    'allows_null': 7,
    'uses_ontology': 9,
    'field_length': 10,
    'valid_values_long': 12,
    'llm_curate': 13,
    'llm_instructions': 14,
}

# Type mapping for pydantic generation
TYPE_MAP = {
    'VARCHAR': 'str',
    'TEXT': 'str',
    'INT': 'int',
    'FLOAT': 'float',
    'BOOLEAN': 'bool',
    'DATE': 'date',
    'DATETIME': 'datetime',
    'ENUM': 'str',  # Will be replaced with Literal
}


def parse_xlsx(xlsx_path: Path) -> dict:
    """Parse xlsx and return dict grouped by model name, filtering out PK/FK."""
    wb = load_workbook(xlsx_path)
    ws = wb['data_dictionary']

    models = defaultdict(lambda: {'fields': {}})

    for row in ws.iter_rows(min_row=2, values_only=True):
        class_name = row[COLS['django_class_name']]
        field_name = row[COLS['django_field_name']]
        key = row[COLS['key']]

        # Skip if no class name, no field name, or is PK/FK
        if not class_name or not field_name:
            continue
        if key and key.upper() in ('PK', 'FK'):
            continue

        # Parse valid_values_long into list if present
        valid_values = row[COLS['valid_values_long']]
        if valid_values and valid_values != 'NA':
            valid_values = parse_enum_values(valid_values)
        else:
            valid_values = None

        # Parse field_length
        field_length = row[COLS['field_length']]
        if field_length == 'NA' or field_length is None:
            field_length = None
        elif isinstance(field_length, str):
            try:
                field_length = int(field_length)
            except ValueError:
                field_length = None

        # Parse booleans
        allows_null = str(row[COLS['allows_null']]).lower() == 'yes'
        uses_ontology = str(row[COLS['uses_ontology']]).lower() == 'yes'
        llm_curate = str(row[COLS['llm_curate']]).upper() == 'YES'

        models[class_name]['fields'][field_name] = {
            'description': row[COLS['description']] or '',
            'data_type': row[COLS['data_type']] or 'VARCHAR',
            'allows_null': allows_null,
            'field_length': field_length,
            'valid_values_long': valid_values,
            'uses_ontology': uses_ontology,
            'llm_curate': llm_curate,
            'llm_instructions': row[COLS['llm_instructions']] or None,
        }

    return dict(models)


def parse_enum_values(value_str: str) -> list:
    """Parse enum values from string like '[val1,val2]' or multiline."""
    if not value_str:
        return []

    # Remove brackets if present
    value_str = value_str.strip()
    if value_str.startswith('[') and value_str.endswith(']'):
        value_str = value_str[1:-1]

    # Split by comma or newline
    values = []
    for part in re.split(r'[,\n]', value_str):
        part = part.strip()
        if part:
            values.append(part)

    return values


def generate_yaml(models: dict, output_path: Path, source_file: str) -> None:
    """Write models dict to YAML file with source header."""
    with open(output_path, 'w') as f:
        f.write(f"# Auto-generated from: {source_file}\n")
        f.write("# Do not edit manually - regenerate from xlsx source.\n\n")
        yaml.dump(models, f, default_flow_style=False, sort_keys=False, allow_unicode=True)
    print(f"Generated: {output_path}")


def generate_pydantic_models(models: dict, output_path: Path, source_file: str) -> None:
    """Generate pydantic model Python code from models dict."""
    lines = [
        '"""',
        'Auto-generated Pydantic models for ASCR curation.',
        '',
        f'Source: {source_file}',
        'Generated by: make_data_dictionary.py',
        'Do not edit manually - regenerate from xlsx source.',
        '"""',
        '',
        'from datetime import date, datetime',
        'from typing import List, Literal, Optional',
        '',
        'from pydantic import BaseModel, Field',
        '',
        '',
    ]

    # Generate individual models
    for model_name, model_def in models.items():
        lines.append(f'class {model_name}(BaseModel):')

        fields = model_def.get('fields', {})
        if not fields:
            lines.append('    pass')
            lines.append('')
            lines.append('')
            continue

        for field_name, field_def in fields.items():
            field_line = generate_field_line(field_name, field_def)
            lines.append(f'    {field_line}')

        lines.append('')
        lines.append('')

    # Generate composite CellLineCurationForm model
    lines.append('class CellLineCurationForm(BaseModel):')
    lines.append('    """Composite model containing all cell line curation data."""')

    # Track field names to avoid collisions
    seen_field_names = set()
    for model_name in models.keys():
        # Convert to snake_case (no pluralization - mirrors table names)
        s1 = re.sub('(.)([A-Z][a-z]+)', r'\1_\2', model_name)
        field_name = re.sub('([a-z0-9])([A-Z])', r'\1_\2', s1).lower()

        # Handle collisions by appending _list suffix
        if field_name in seen_field_names:
            field_name = field_name + '_list'

        seen_field_names.add(field_name)
        lines.append(f'    {field_name}: List[{model_name}] = Field(default_factory=list)')

    lines.append('')
    lines.append('')

    with open(output_path, 'w') as f:
        f.write('\n'.join(lines))
    print(f"Generated: {output_path}")


def generate_field_line(field_name: str, field_def: dict) -> str:
    """Generate a single pydantic field line."""
    data_type = field_def['data_type']
    allows_null = field_def['allows_null']
    field_length = field_def['field_length']
    valid_values = field_def['valid_values_long']
    description = field_def['description']

    # Determine Python type
    if data_type == 'ENUM' and valid_values:
        # Use Literal for enums
        escaped_values = [f'"{v}"' for v in valid_values]
        py_type = f'Literal[{", ".join(escaped_values)}]'
    else:
        py_type = TYPE_MAP.get(data_type, 'str')

    # Build Field() arguments
    field_args = []

    if allows_null:
        py_type = f'Optional[{py_type}]'
        field_args.append('default=None')

    if field_length and data_type in ('VARCHAR', 'TEXT'):
        field_args.append(f'max_length={field_length}')

    if description:
        # Escape quotes in description
        desc_escaped = description.replace('"', '\\"')
        field_args.append(f'description="{desc_escaped}"')

    # Build the line
    if field_args:
        return f'{field_name}: {py_type} = Field({", ".join(field_args)})'
    else:
        return f'{field_name}: {py_type}'


def generate_llm_instructions(models: dict, output_path: Path, source_file: str) -> None:
    """Generate markdown file with LLM curation instructions."""
    lines = [
        "# LLM Curation Instructions",
        "",
        f"Auto-generated from: {source_file}",
        "",
        "Fields marked for LLM curation (`llm_curate = YES`).",
        "",
    ]

    for model_name, model_def in models.items():
        fields = model_def.get('fields', {})
        llm_fields = [(name, f) for name, f in fields.items() if f.get('llm_curate')]

        if not llm_fields:
            continue

        lines.append(f"## {model_name}")
        lines.append("")
        lines.append("| Field | Instruction |")
        lines.append("|-------|-------------|")

        for field_name, field_def in llm_fields:
            instruction = field_def.get('llm_instructions') or '_No instruction provided_'
            # Escape pipe characters and newlines for markdown table
            instruction = instruction.replace('|', '\\|').replace('\n', ' ')
            lines.append(f"| `{field_name}` | {instruction} |")

        lines.append("")

    # Ensure output directory exists
    output_path.parent.mkdir(parents=True, exist_ok=True)

    with open(output_path, 'w') as f:
        f.write('\n'.join(lines))
    print(f"Generated: {output_path}")


def generate_json_schema(models_path: Path, output_path: Path, source_file: str) -> None:
    """Import generated models and export JSON schema as .jsonc with comments."""
    import sys
    import importlib.util

    # Load the generated module
    spec = importlib.util.spec_from_file_location("curation_models", models_path)
    module = importlib.util.module_from_spec(spec)
    sys.modules["curation_models"] = module
    spec.loader.exec_module(module)

    # Collect all model classes
    from pydantic import BaseModel

    schemas = {}
    for name in dir(module):
        obj = getattr(module, name)
        if isinstance(obj, type) and issubclass(obj, BaseModel) and obj is not BaseModel:
            schemas[name] = obj.model_json_schema()

    # Write combined schema
    combined = {
        '$schema': 'https://json-schema.org/draft/2020-12/schema',
        'title': 'ASCR Curation Schema',
        'description': 'JSON schema for cell line curation forms',
        '$defs': schemas,
    }

    with open(output_path, 'w') as f:
        f.write(f"// Auto-generated from: {source_file}\n")
        f.write("// Do not edit manually - regenerate from xlsx source.\n")
        json.dump(combined, f, indent=2)
    print(f"Generated: {output_path}")


def main():
    parser = argparse.ArgumentParser(description='Generate curation artifacts from data dictionary')
    parser.add_argument(
        '--input',
        type=Path,
        default=Path('data_dictionaries/2025_12_ascr_data_dictionary_v1.0.xlsx'),
        help='Input xlsx file path'
    )
    parser.add_argument(
        '--output-dir',
        type=Path,
        default=Path('data_dictionaries'),
        help='Output directory for generated files'
    )
    args = parser.parse_args()

    # Paths
    yaml_path = args.output_dir / 'curation_schema.yaml'
    py_path = args.output_dir / 'curation_models.py'
    json_path = args.output_dir / 'curation_schema.jsonc'
    instructions_path = Path('curation_instructions') / 'llm_curation_instructions.md'

    # Source file name for headers
    source_file = args.input.name

    # Step 1: Parse xlsx
    print(f"Parsing: {args.input}")
    models = parse_xlsx(args.input)
    print(f"Found {len(models)} models")

    # Step 2: Generate YAML
    generate_yaml(models, yaml_path, source_file)

    # Step 3: Generate pydantic models
    generate_pydantic_models(models, py_path, source_file)

    # Step 4: Generate JSON schema
    generate_json_schema(py_path, json_path, source_file)

    # Step 5: Generate LLM curation instructions
    generate_llm_instructions(models, instructions_path, source_file)

    print("Done!")


if __name__ == '__main__':
    main()
